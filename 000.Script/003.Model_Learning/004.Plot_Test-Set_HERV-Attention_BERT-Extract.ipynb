{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "file=\"/home/u20111010010/Project/DNA-Pretraining/Level1/001.Genomics_dataset/Dataset_HERV/VCF_hprc-1000G/Train_Test/data_all_model_HERV-Classification_Need.fa\"\n",
    "df1=pd.read_csv(file,sep=\"\\t\",header=None).rename(columns = {0: \"dset\", 1: \"multi\",2:\"binary\", 3: \"seq\",4:\"Type\",5: \"detail\"})\n",
    "df = df1.loc[:, ['dset', 'multi','seq']]\n",
    "df = df[df['dset'] == 'test']\n",
    "\n",
    "print(\"+++++++++++++++++++++++++++++Test sets\")\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "### label间转换\n",
    "id2label={\"0\":\"Non-HERV_Coding\",\"1\":\"HERV_Coding\",\"2\":\"Non-HERV_Non-Coding\",\"3\":\"HERV_Non-Coding\"}\n",
    "labels_raw=list(df['multi'])\n",
    "labels = [id2label[str(i)] for i in labels_raw]\n",
    "\n",
    "print(Counter(labels))\n",
    "\n",
    "sequences=list(df['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"BERT_HERV_Multi_RUN0\"\n",
    "output_dir=\"/home/u20111010010/Project/DNA-Pretraining/Level1/003.Sequence_Visualization/Dataset_HERV\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型学习到的Attention矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scores(attention_scores, kmer):\n",
    "    # 初始化两个全零数组，用于存储处理后的注意力分数\n",
    "    scores = np.zeros([attention_scores.shape[0], attention_scores.shape[-1]])\n",
    "    unnorm = np.zeros([attention_scores.shape[0], attention_scores.shape[-1]])\n",
    "\n",
    "    # 遍历每个样本的注意力分数\n",
    "    # attention_scores: (batch_size, num_heads, seq_len, seq_len)\n",
    "    for index, attention_score in enumerate(attention_scores):\n",
    "        # 初始化一个空列表来存储每个位置的总分数\n",
    "        attn_score = []\n",
    "        # 从第二个位置开始遍历到最后一个位置\n",
    "        for i in range(1, attention_score.shape[-1] - kmer + 2):\n",
    "            # 对每个位置，累加所有头的分数，并将结果添加到 attn_score 列表中\n",
    "            attn_score.append(float(attention_score[:, 0, i].sum()))\n",
    "\n",
    "        # 如果后一个分数为0，将前一个分数设置为0，并退出循环\n",
    "        for i in range(len(attn_score) - 1):\n",
    "            if attn_score[i + 1] == 0:\n",
    "                attn_score[i] = 0\n",
    "                break\n",
    "\n",
    "        # 初始化两个数组，一个用于计数，另一个用于存储真实的分数\n",
    "        counts = np.zeros([len(attn_score) + kmer - 1])\n",
    "        real_scores = np.zeros([len(attn_score) + kmer - 1])\n",
    "        # 遍历 attn_score，并更新 counts 和 real_scores\n",
    "        for i, score in enumerate(attn_score):\n",
    "            for j in range(kmer):\n",
    "                counts[i + j] += 1.0\n",
    "                real_scores[i + j] += score\n",
    "        # 计算真实的平均分数\n",
    "        real_scores = real_scores / counts\n",
    "        # 存储未归一化的分数\n",
    "        unnorm[index] = real_scores\n",
    "        # 对分数进行L2归一化\n",
    "        real_scores = real_scores / np.linalg.norm(real_scores)\n",
    "        # 存储归一化后的分数\n",
    "        scores[index] = real_scores\n",
    "        \n",
    "    return scores, unnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multi_score(attention_scores, kmer):\n",
    "    # 初始化三维全零数组，用于存储每个样本、每个头的处理后的注意力分数\n",
    "    scores = np.zeros(\n",
    "        [\n",
    "            attention_scores.shape[0],\n",
    "            attention_scores.shape[1],\n",
    "            attention_scores.shape[-1],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 遍历每个样本的注意力分数\n",
    "    # attention_scores: (batch_size, num_heads, seq_len, seq_len)\n",
    "    for index, attention_score in enumerate(attention_scores):\n",
    "        # 初始化二维数组，用于存储每个头的处理后的注意力分数\n",
    "        head_scores = np.zeros([attention_scores.shape[1], attention_scores.shape[-1]])\n",
    "        \n",
    "        # 遍历每个注意力头\n",
    "        for head in range(0, len(attention_score)):\n",
    "            attn_score = []\n",
    "\n",
    "            # 从第二个位置开始遍历到最后一个位置，计算该头的每个位置的总分数\n",
    "            for i in range(1, attention_score.shape[-1] - kmer + 2):\n",
    "                attn_score.append(float(attention_score[head, 0, i]))\n",
    "\n",
    "            # 如果后一个分数为0，将前一个分数设置为0，并退出循环\n",
    "            for i in range(len(attn_score) - 1):\n",
    "                if attn_score[i + 1] == 0:\n",
    "                    attn_score[i] = 0\n",
    "                    break\n",
    "\n",
    "            # 初始化两个数组，一个用于计数，另一个用于存储真实的分数\n",
    "            counts = np.zeros([len(attn_score) + kmer - 1])\n",
    "            real_scores = np.zeros([len(attn_score) + kmer - 1])\n",
    "            \n",
    "            # 遍历 attn_score，并更新 counts 和 real_scores\n",
    "            for i, score in enumerate(attn_score):\n",
    "                for j in range(kmer):\n",
    "                    counts[i + j] += 1.0\n",
    "                    real_scores[i + j] += score\n",
    "            \n",
    "            # 计算真实的平均分数\n",
    "            real_scores = real_scores / counts\n",
    "            # 对分数进行L2归一化\n",
    "            real_scores = real_scores / np.linalg.norm(real_scores)\n",
    "            \n",
    "            # 存储该头的处理后的分数\n",
    "            head_scores[head] = real_scores\n",
    "\n",
    "        # 存储该样本的每个头的处理后的分数\n",
    "        scores[index] = head_scores\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 初始化模型和分词器，并将模型移动到GPU上\n",
    "model_name = \"/home/u20111010010/Project/DNA-Pretraining/Level1/002.Model_Classification/Dataset_HERV/Model/BERT_HERV_Multi_RUN0\"\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 6-mers编码\n",
    "def kmers(s, k=6):\n",
    "    return [s[i:i + k] for i in range(0, len(s)-k+1)]\n",
    "\n",
    "def transform_seq(seq):\n",
    "    return \" \".join(kmers(seq)[:300] + kmers(seq)[-212:])\n",
    "\n",
    "def pad_to_length(arr, target_length, axis=-1):\n",
    "    \"\"\"\n",
    "    用0填充数组以达到目标长度。\n",
    "    \"\"\"\n",
    "    padding_shape = list(arr.shape)\n",
    "    padding_shape[axis] = target_length - arr.shape[axis]\n",
    "    padding = np.zeros(padding_shape, dtype=arr.dtype)\n",
    "    return np.concatenate([arr, padding], axis=axis)\n",
    "\n",
    "# 特征提取\n",
    "def extract_attentions(model, tokenizer, sequences, batch_size=32):\n",
    "    \"\"\"\n",
    "    使用给定的模型和分词器从序列中提取特征，分批进行预测。\n",
    "    :param model: 使用的模型\n",
    "    :param tokenizer: 使用的分词器\n",
    "    :param sequences: 序列列表\n",
    "    :param batch_size: 每个批次的序列数量\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    score_len=512\n",
    "    num_sequences = len(sequences)\n",
    "\n",
    "    single_attentions = np.zeros((num_sequences, score_len))\n",
    "    unnorm_attentions = np.zeros((num_sequences, score_len))\n",
    "    #pred_results = np.zeros((len(sequences), num_labels))\n",
    "    multi_attentions = np.zeros((num_sequences, 12, score_len))\n",
    "\n",
    "\n",
    "    for i in range(0,num_sequences, batch_size):  # 每batch_size个序列进行一次预测\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # 获取实际批量大小\n",
    "            actual_batch_size = min(batch_size, num_sequences - i)\n",
    "            batch_sequences = sequences[i:i+actual_batch_size]\n",
    "\n",
    "            # 批量编码\n",
    "            transformed_reads = [transform_seq(seq) for seq in batch_sequences]\n",
    "            inputs = tokenizer(transformed_reads, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            outputs = model(**inputs,output_attentions=True)\n",
    "\n",
    "            # 保存注意力分数,最后一层的注意力矩阵。这个注意力矩阵是与最后一层的隐藏层相对应的。这通常是模型中最有信息量的层，因为它捕获了所有前面层的信息\n",
    "            out_attns = (outputs.attentions[-1]).cpu().numpy()\n",
    "            #out_attns = outputs[\"attentions\"].cpu().numpy()\n",
    "            \n",
    "            single_attn, unnormed_attn = process_scores(out_attns,kmer=6)\n",
    "            multi_attn = process_multi_score(out_attns,kmer=6)\n",
    "\n",
    "            #  确保数据具有正确的形状\n",
    "            single_attn = pad_to_length(single_attn, score_len)\n",
    "            unnormed_attn = pad_to_length(unnormed_attn, score_len)\n",
    "            multi_attn = pad_to_length(multi_attn, score_len, axis=-1)\n",
    "\n",
    "            # 注意确保这里的形状匹配\n",
    "            single_attentions[i:i + actual_batch_size, :] = single_attn[:actual_batch_size, :]\n",
    "            unnorm_attentions[i:i + actual_batch_size, :] = unnormed_attn[:actual_batch_size, :]\n",
    "            multi_attentions[i:i + actual_batch_size, :, :] = multi_attn[:actual_batch_size, :, :]\n",
    "     \n",
    "    return single_attentions,unnorm_attentions,multi_attentions\n",
    "\n",
    "\n",
    "### 行数等于输入序列的数量,列数等于模型的last_hidden_state中的特征数量(768)\n",
    "single,unnorm,multi = extract_attentions(model, tokenizer, sequences)\n",
    "np.save(os.path.join(output_dir, \"BERT_HERV_Multi_RUN0_extract_single_attentions.npy\"), single)\n",
    "np.save(os.path.join(output_dir, \"BERT_HERV_Multi_RUN0_extract_unnorm_attentions.npy\"), unnorm)\n",
    "np.save(os.path.join(output_dir, \"BERT_HERV_Multi_RUN0_extract_multi_attentions.npy\"), multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.1.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
